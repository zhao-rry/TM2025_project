{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95155ead-60d5-449b-88e5-83af88a96aff",
   "metadata": {},
   "source": [
    "<h1> Quantitative Analysis </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3476e65f-223d-46e7-b9b3-65378a138c10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dd8f20-6949-4bd4-a118-56344e73210f",
   "metadata": {},
   "source": [
    "<h2> Preprocessing Steps </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59feb18b-ca3e-44f3-a6b6-5d423176a308",
   "metadata": {},
   "source": [
    "__Step 1:__\n",
    "\n",
    "Read data into Pandas dataframe and examine the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d4419ed-79bf-462f-8d79-836b7ac158ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   letter    year language\n",
      "arc71   Mercredi 29 septembre 1915\\nMa chère Louisette...  1915.0   french\n",
      "hl_01   Magnac Laval\\n\\nChère épouse et parents,\\n\\nAu...     NaN   french\n",
      "hl_02   Correspondance militaire adressée à monsieur J...  1914.0   french\n",
      "hl_03a  Chère femme, mes deux gosses ainsi que toute m...  1914.0   french\n",
      "hl_03b  Chers Mère et frère,\\n\\nTout ce que je vous re...  1914.0   french \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 78 entries, arc71 to new3\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   letter    78 non-null     object \n",
      " 1   year      55 non-null     float64\n",
      " 2   language  78 non-null     object \n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 4.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "RAW_FILE_PATH = Path.cwd() / \"ww1letters.json\"\n",
    "RAW_METADATA_PATH = Path.cwd() / \"index.csv\"\n",
    "\n",
    "# we should include metadata such as the year, language, author\n",
    "with open(RAW_FILE_PATH, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "    indices = json_data.keys()\n",
    "    letters = json_data.values()\n",
    "    data_df = pd.DataFrame({\n",
    "        \"letter\": letters\n",
    "    }, index = indices)\n",
    "\n",
    "with open(RAW_METADATA_PATH, 'r') as f:\n",
    "    metadata_df = pd.read_csv(f)\n",
    "    metadata_df.index = metadata_df[\"letter_key\"]\n",
    "\n",
    "data_df = data_df.join(\n",
    "    metadata_df[[\"year\", \"language\"]],\n",
    ")\n",
    "data_df[\"language\"] = data_df[\"language\"].fillna(\"english\")\n",
    "\n",
    "print(data_df.head(), \"\\n\")\n",
    "print(data_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fade4a8-43fd-46d4-bac1-d2e88ee729a7",
   "metadata": {},
   "source": [
    "__Step 2:__\n",
    "\n",
    "Retrieve English letters and apply tokenisation, lemmatisation, other steps?\n",
    "\n",
    "How to deal with French letters?\n",
    "\n",
    "We'll also explain more steps here.\n",
    "\n",
    "credits to notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7177ee4a-6f91-4321-b42d-33510e3f966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "STEMMER = nltk.stem.WordNetLemmatizer()\n",
    "un_to_wn_map = {\"VERB\" : wordnet.VERB,\n",
    "                \"NOUN\" : wordnet.NOUN,\n",
    "                \"ADJ\" : wordnet.ADJ,\n",
    "                \"ADV\" : wordnet.ADV}\n",
    "PUNCTUATIONS = {'’', '“', '”', '[', ']'}\n",
    "\n",
    "english_letters = data_df[\"letter\"][data_df[\"language\"] == \"english\"]\n",
    "# print(english_letters)\n",
    "\n",
    "letters_preprocessed = []\n",
    "for letter in english_letters:\n",
    "    letter_tokens = nltk.tokenize.word_tokenize(letter.lower())\n",
    "    letter_tagged = nltk.pos_tag(letter_tokens, tagset = \"universal\")\n",
    "    letter_lemmas = []\n",
    "    \n",
    "    for (token, pos) in letter_tagged:\n",
    "        if pos in un_to_wn_map.keys() and token not in PUNCTUATIONS:\n",
    "            letter_lemmas.append(STEMMER.lemmatize(token,\n",
    "                                                  pos = un_to_wn_map[pos]))\n",
    "        elif token.isalnum():\n",
    "            letter_lemmas.append(STEMMER.lemmatize(token))\n",
    "\n",
    "    letters_preprocessed.append(letter_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60777cdd-416a-4aa4-a1d3-1951b75aa790",
   "metadata": {},
   "source": [
    "__Step 3:__\n",
    "\n",
    "Replace the letters in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5295b135-33ca-4a85-9554-9fbd5f1edfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     letter    year\n",
      "na_uk_01  [dear, mr, welsh, we, be, only, out, here, a, ...  1915.0\n",
      "na_uk_02  [dear, nic, boyce, thanks, very, much, for, yo...  1915.0\n",
      "na_uk_03  [dear, lack, many, thanks, for, the, photograp...  1915.0\n",
      "na_uk_04  [dear, bert, just, a, few, line, to, let, you,...  1915.0\n",
      "na_uk_05  [dear, mr, hunt, please, accept, yourself, and...  1915.0 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 58 entries, na_uk_01 to new3\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   letter  58 non-null     object \n",
      " 1   year    36 non-null     float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 1.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "english_data_df = data_df[data_df[\"language\"] == \"english\"].copy()\n",
    "english_data_df = english_data_df.drop(columns = [\"language\"])\n",
    "english_data_df[\"letter\"] = letters_preprocessed\n",
    "\n",
    "print(english_data_df.head(), \"\\n\")\n",
    "print(english_data_df.info())\n",
    "\n",
    "english_data_df.to_csv(Path.cwd() / \"preprocessed_letters.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
