{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "97c5f53c-7a8f-4655-bb80-a303ca0a8152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.11)\n",
      "Path to model files: C:\\Users\\kevin\\.cache\\kagglehub\\models\\omlande\\bert-multiclassification-sentiment-analysis\\pyTorch\\default\\1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nModelConfigPath = path + \"\\\\bert_emotion_classifier\"\\nModelConfig = json.loads(ModelPath)\\nTokenizerPath = ModelPath + \"\\\\tokenizer_config.json\"\\nTokenizerConfig = json.loads(TokenizerPath)\\n'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kagglehub\n",
    "import json\n",
    "from transformers import AutoConfig, BertModel, PreTrainedTokenizer, BertConfig, AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.model_download(\"omlande/bert-multiclassification-sentiment-analysis/pyTorch/default\")\n",
    "print(\"Path to model files:\", path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert_emotion_classifier\")\n",
    "model = BertModel.from_pretrained(\"bert_emotion_classifier\", use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f01bf4b1-9c50-4fda-93a4-af2085faa50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word, layer_nums):\n",
    "    # Tokenize the word into subtokens and add special tokens [CLS] and [SEP]\n",
    "    subtokens = [tokenizer.cls_token] + tokenizer.tokenize(word) + [tokenizer.sep_token]\n",
    "    # Convert subtokens to input IDs\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(subtokens)\n",
    "    # Wrap it in a tensor and add an extra batch dimension\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "    # Make sure the model does not compute gradients\n",
    "    with torch.no_grad():\n",
    "        # Get the model outputs\n",
    "        outputs = model(input_ids, output_hidden_states=True)\n",
    "    # Check if layer_nums is a list or a single integer\n",
    "    if isinstance(layer_nums, int):\n",
    "        layer_nums = [layer_nums]\n",
    "    # Use the hidden state from the specified layers as word embedding\n",
    "    embeddings = [outputs.hidden_states[i] for i in layer_nums]\n",
    "    # Average the embeddings from the specified layers\n",
    "    averaged_embedding = torch.mean(torch.stack(embeddings), dim=0)\n",
    "    # Ignore the first and the last token ([CLS] and [SEP])\n",
    "    averaged_embedding = averaged_embedding[0, 1:-1]\n",
    "    # Get the mean of the subtoken vectors to get the word vector\n",
    "    word_embedding = torch.mean(averaged_embedding, dim=0)\n",
    "    # Convert tensor to a numpy array\n",
    "    word_embedding = word_embedding.numpy()\n",
    "    return word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b1ea2c17-a74b-4881-a859-21f8ea897387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(word1, word2, layer_nums):\n",
    "    word1_embedding = get_word_embedding(word1, layer_nums)\n",
    "    word2_embedding = get_word_embedding(word2, layer_nums)\n",
    "    similarity = 1 - cosine(word1_embedding, word2_embedding)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "5c7cceba-9582-4498-affb-62cc803909e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'happy'\t'happy'\t1.00\n",
      "'happy'\t'elated'\t0.12\n",
      "'depressed'\t'sad'\t0.32\n",
      "'car'\t'minivan'\t0.24\n",
      "'car'\t'communism'\t0.10\n"
     ]
    }
   ],
   "source": [
    "# similarity queries (default to cosine similarity: 0 least similar, to 1 most similar)\n",
    "pairs = [\n",
    "    ('happy', 'happy'),   # a minivan is a kind of car\n",
    "    ('happy', 'elated'),   # still a wheeled vehicle\n",
    "    ('depressed', 'sad'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'minivan'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, calculate_similarity(w1, w2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d60fd60-8a45-47b1-b65d-e954c6d6eb91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
