{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95155ead-60d5-449b-88e5-83af88a96aff",
   "metadata": {},
   "source": [
    "<h1> Quantitative Analysis </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3476e65f-223d-46e7-b9b3-65378a138c10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dd8f20-6949-4bd4-a118-56344e73210f",
   "metadata": {},
   "source": [
    "<h2> Preprocessing Steps </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59feb18b-ca3e-44f3-a6b6-5d423176a308",
   "metadata": {},
   "source": [
    "<h4>Step 1</h4>\n",
    "\n",
    "Read data into Pandas dataframe and examine the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d4419ed-79bf-462f-8d79-836b7ac158ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   letter  year language\n",
      "arc71   Mercredi 29 septembre 1915\\nMa chère Louisette...  1915   french\n",
      "hl_02   Correspondance militaire adressée à monsieur J...  1914   french\n",
      "hl_03a  Chère femme, mes deux gosses ainsi que toute m...  1914   french\n",
      "hl_03b  Chers Mère et frère,\\n\\nTout ce que je vous re...  1914   french\n",
      "hl_04   Aux armées le 27 mai 1916\\n\\n \\nChers parents,...  1916   french \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 55 entries, arc71 to na_uk_40\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   letter    55 non-null     object\n",
      " 1   year      55 non-null     int64 \n",
      " 2   language  55 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_json_to_df(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a file path, opens the .json file at\n",
    "    that position, and transforms it into a dataframe\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        return pd.DataFrame({\n",
    "            \"letter\": json_data.values()\n",
    "        }, index = json_data.keys())\n",
    "\n",
    "DATA_DIR = Path.cwd() / \"data\"\n",
    "KAGGLE_FILE_PATH = DATA_DIR / \"ww1letters.json\"\n",
    "KAGGLE_METADATA_PATH = DATA_DIR / \"index.csv\"\n",
    "\n",
    "kaggle_df = load_json_to_df(KAGGLE_FILE_PATH)\n",
    "with open(KAGGLE_METADATA_PATH, 'r') as f:\n",
    "    metadata_df = pd.read_csv(f)\n",
    "    metadata_df.index = metadata_df[\"letter_key\"]\n",
    "\n",
    "kaggle_df = kaggle_df.join(\n",
    "    metadata_df[[\"year\", \"language\"]],\n",
    ")\n",
    "kaggle_df = kaggle_df[kaggle_df[\"year\"].notna()].astype({\"year\": \"int64\"})\n",
    "kaggle_df[\"language\"] = kaggle_df[\"language\"].fillna(\"english\")\n",
    "\n",
    "print(kaggle_df.head(), \"\\n\")\n",
    "print(kaggle_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc0abd5-e98f-473f-bffc-ac3639c92643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  letter  year\n",
      "lo001  Dear Ginger, I have just returned from a holid...  1915\n",
      "lo002  Zeppelins over London 4th Nov 1915 David to Gi...  1915\n",
      "lo003  David Joins Army 28 Dec 1915 David to Ginger D...  1915\n",
      "lo004  How Britain Prepared Ginger (Ethel)  to David ...  1916\n",
      "lo005  1916 OTC Gidea Park David to Ginger (sister Et...  1916 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 367 entries, lo001 to sh015\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   letter  367 non-null    object\n",
      " 1   year    367 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 8.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "DEAREST_PATH = DATA_DIR / \"400_letters.json\"\n",
    "\n",
    "dearest_df = load_json_to_df(DEAREST_PATH)\n",
    "\n",
    "london_years = [1915] * 3 + [1916] * 6\n",
    "france_years = [1917] * 77\n",
    "missing_years = [1917] * 13\n",
    "pow_years = [1918] * 174 + [1917] * 79\n",
    "sheerness_years = [1919] * 15\n",
    "\n",
    "dearest_df[\"year\"] = london_years + france_years + missing_years + pow_years + sheerness_years\n",
    "print(dearest_df.head(), \"\\n\")\n",
    "print(dearest_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fade4a8-43fd-46d4-bac1-d2e88ee729a7",
   "metadata": {},
   "source": [
    "<h4>Step 2</h4>\n",
    "\n",
    "Retrieve English letters and apply tokenisation, lemmatisation, other steps?\n",
    "\n",
    "How to deal with French letters?\n",
    "\n",
    "We'll also explain more steps here.\n",
    "\n",
    "credits to notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7177ee4a-6f91-4321-b42d-33510e3f966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from collections import Counter\n",
    "\n",
    "STEMMER = nltk.stem.WordNetLemmatizer()\n",
    "un_to_wn_map = {\"VERB\" : wordnet.VERB,\n",
    "                \"NOUN\" : wordnet.NOUN,\n",
    "                \"ADJ\" : wordnet.ADJ,\n",
    "                \"ADV\" : wordnet.ADV}\n",
    "\n",
    "PUNCTUATIONS = {',', '.', ';', '?', '!', '&', '(', ')', '*', '-', '‘', '’', '“', '”', '[', ']', '…', '+'}\n",
    "STOP = set(stopwords.words(\"english\")).union(PUNCTUATIONS)\n",
    "\n",
    "english_kaggle = kaggle_df[\"letter\"][kaggle_df[\"language\"] == \"english\"]\n",
    "# print(english_kaggle)\n",
    "\n",
    "def preprocess(df: pd.DataFrame, common_removed = 0.0) -> (list[str], Counter):\n",
    "    \"\"\"\n",
    "    Takes a series of letters and preprocesses them\n",
    "    by applying tokenisation, pos-tagging & lemmatisation.\n",
    "    \n",
    "    Removes the top <common_removed>% of most frequent\n",
    "    words from the preprocessed results.\n",
    "    \n",
    "    Returns the preprocessed letters and a counter of\n",
    "    word frequencies.\n",
    "    \"\"\"\n",
    "    letters_preprocessed = []\n",
    "    word_freqs = Counter()\n",
    "    \n",
    "    for letter in df:\n",
    "        letter_tokens = nltk.tokenize.word_tokenize(letter.lower())\n",
    "        letter_tagged = nltk.pos_tag(letter_tokens, tagset = \"universal\")\n",
    "        letter_lemmas = []\n",
    "        \n",
    "        for (token, pos) in letter_tagged:\n",
    "            token = token.strip('…')\n",
    "            if token not in STOP:\n",
    "                if pos in un_to_wn_map.keys():\n",
    "                    lemma = STEMMER.lemmatize(token, pos = un_to_wn_map[pos])\n",
    "                else:\n",
    "                    lemma = STEMMER.lemmatize(token)\n",
    "                \n",
    "                letter_lemmas.append(lemma)\n",
    "                word_freqs[lemma] += 1\n",
    "        \n",
    "        letters_preprocessed.append(' '.join(letter_lemmas))\n",
    "\n",
    "    num_remove = int(common_removed * len(word_freqs))\n",
    "    words_remove = set([\n",
    "        item[0] for item in word_freqs.most_common(num_remove)\n",
    "    ])\n",
    "    \n",
    "    letters_preprocessed = [\n",
    "        ' '.join([word for word in letter.split(' ')\n",
    "                  if word not in words_remove])\n",
    "        for letter in letters_preprocessed\n",
    "    ]\n",
    "\n",
    "    return (letters_preprocessed, word_freqs)\n",
    "\n",
    "REMOVE = 0.02\n",
    "english_kaggle_pp, english_freqs = preprocess(english_kaggle, REMOVE)\n",
    "dearest_pp, dearest_freqs = preprocess(dearest_df[\"letter\"], REMOVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60777cdd-416a-4aa4-a1d3-1951b75aa790",
   "metadata": {},
   "source": [
    "<h4>Step 3</h4>\n",
    "\n",
    "Replace the letters in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5295b135-33ca-4a85-9554-9fbd5f1edfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  letter  year\n",
      "lo001  ginger return holiday brighton glorious chesha...  1915\n",
      "lo002  zeppelin london 4th nov 1915 ginger 56 ramsden...  1915\n",
      "lo003  join army 28 dec 1915 ginger dec 28th 1915 ram...  1915\n",
      "lo004  britain prepared ginger somewhere u.s.a. somet...  1916\n",
      "lo005  1916 otc gidea park ginger sister live usa cad...  1916 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 403 entries, lo001 to na_uk_40\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   letter  403 non-null    object\n",
      " 1   year    403 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 9.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "english_data_df = kaggle_df[kaggle_df[\"language\"] == \"english\"].copy()\n",
    "english_data_df = english_data_df.drop(columns = [\"language\"])\n",
    "english_data_df[\"letter\"] = english_kaggle_pp\n",
    "\n",
    "dearest_df[\"letter\"] = dearest_pp\n",
    "\n",
    "data_df = pd.concat([dearest_df, english_data_df], ignore_index = False)\n",
    "\n",
    "print(data_df.head(), \"\\n\")\n",
    "print(data_df.info())\n",
    "\n",
    "data_df.to_csv(DATA_DIR / \"preprocessed_letters.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5196b4-b9aa-466c-85ca-32d71fb150dd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb5d805-5363-4fa5-9751-a14401db3e74",
   "metadata": {},
   "source": [
    "<h2>General Statistics</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e353719-efb3-423a-803f-c3f28b3203b5",
   "metadata": {},
   "source": [
    "<h4>ugh</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a246d878-f2be-42de-91f5-bb1f85be52ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path.cwd() / \"data\"\n",
    "REMOVE = 0.02\n",
    "letters = pd.read_csv(DATA_DIR / \"preprocessed_letters.csv\")[\"letter\"]\n",
    "\n",
    "DATA_SIZE = len(letters)\n",
    "size_measures = {\n",
    "    \"text size\": [],\n",
    "    \"vocab size\": [],\n",
    "    \"type-token ratio\": []\n",
    "}\n",
    "total_freq = Counter()\n",
    "    \n",
    "for letter in letters:\n",
    "    tokens = letter.split(' ')\n",
    "    text_size = len(tokens)\n",
    "    vocab_size = len(set(tokens))\n",
    "    \n",
    "    size_measures[\"text size\"].append(text_size)\n",
    "    size_measures[\"vocab size\"].append(vocab_size)\n",
    "    size_measures[\"type-token ratio\"].append(round(vocab_size / text_size, 2))\n",
    "    \n",
    "    total_freq += Counter(letter.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "903d6e0d-81d2-44f2-9836-c56ce218a15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of letters:  403 \n",
      "\n",
      "---\n",
      "\n",
      "Average text size is 132.33\n",
      "\n",
      "Highest text size is 430\n",
      "CORRESPONDING LETTER:\n",
      ">> zeppelin london 4th nov 1915 ginger 56 ramsden rd balham nov 4th 1915 ginger zeppelin fortunate enough view whole great many happen 9.30. earlier passing zep raid tonight notice report paper somewhere happen common heard boom boom boom gun test something often practice ground bomb throw clapham common almost immediately boom strike zep sky saw burst firing saw beauty behind tree beautiful sight large cigar shape bar silver float along shin light search light fine clear star shin moon pretty dark throw zep gun worth shell burst world star pop hit stood watch sight south east later 12 clock thoroughly amongst suppose return view aldgate large elongated star shell flash light shell near definitely remain minute appear search light hover around outside nearly others comparatively damage although loss life great drop bomb new within 100 yard chancery lane gray inn harry thick tale explosion second hat rush open door saw bomb fall close door bomb explode open door saw bomb fall close door explode zep street full sulphur fume street corner bomb fell close enough blow tail coat chancery lane young river flowing bomb burst water main afterwards brandy holborn woman bring damage bomb chancery lane fell road outside stone building safe deposit smash glass around knock lump stonework burst water main water wood block paving lift 100 yard graceful wave incendiary bomb fell stone building cause fire bomb fell gary inn fell big grass ground close blow window ground floor fitting furniture mix together devil pitch fork stir block gray inn 100 yard spot bomb fell blow glass side building great many internal fanlight crack wall side building blow open casement window wrench fastening case snap cast iron certainly cheap cut clear although glass window sound bomb fell court yard law court knock lump masonry smash window around cheek fell skating rink aldwych mess light temporary building explosion rip whole end end roof truss bare fell road aldwych opposite gaid burst gas main fire burnt considerable spread fell building door lyceum deal damage building stand algate believe bank corner minories smash various window shop pretty smash croydon visit damage whole chiefly glass suffer usual various report panic lyceum woman harry theatre bomb fell entrance door knock burnt quickly audience quietly tale george grosswith sing bomb fell outside safe outside performance sing usual enquire-within-upon-everything gentleman ever ready advice anybody shell burst zep realise difficult hit especially move quickly saw sight within ten minute punch ball kleo highly delight £1-1-0 delay owe fact none stock excuse side account delay account bessie pendle road archie three job huddersfield already bessie follow possible\n",
      "\n",
      "Lowest text size is 7\n",
      "CORRESPONDING LETTER:\n",
      ">> 21st chance card 21 chance censor delay\n",
      "\n",
      "---\n",
      "\n",
      "Average vocab size is 111.61\n",
      "\n",
      "Highest vocab size is 297\n",
      "CORRESPONDING LETTER:\n",
      ">> 1st boot race holzminden 23 1st regard exactly mine plain paint wall infinitely paper every show casement window arrange clean inside regard ventilation upper portion open fanlight regulate open required extent brass tap dote oxidize regard wash basin water seal trap immediately basin smell pipe regard cost set wash stand probably cost pound extra saving worth ideal piece ground piece necessarily 50 0 wide run north south face north south dining drawing face due south straight garden garden larder kitchen road north side case hall kitchen side without alteration arrangement plan extra piece ground west side instead east indeed altogether arrangement point view decoration furnishing bedroom show lead decorator decorate different cox card similar february 1st follow : amount stand credit sum £25 f 56 ramsden road payment london county westminster bank newington before. cox receive card card less month enclose rough sketch beach hut four suggest drawer fit sufficiently large clothes hinge fold wall whole clear dot position kitchen portable range living store necessary trap door loft roof store spare furniture window fold door open direct beach sketch elevation expensive cost great sorry helen husband kill chance world certainly hagell chief draughtsman firm architect use deal use fair amount draw member lodge particularly strike funny helen touch regard gardening grow great part vegetable rough account amount various different roughly require plant adventure contentment davie grayson city life buy farm describe character meet stiff amn million grease cart wheel mad tramp invite live disappear lately altho weather fairly high wind reason whirl wind camp 30 foot diameter 50 foot high column whirl dust fully minute lift ground 100 foot air cloud dust piece paper spin funny especially amongst dozen fellow sit outside scatter direction account dust tremendous change hill entirely cover glorious dark green wood whilst others low portion cut patchwork crop various colour unlike north down remember leith hill whole country shut hill hill stretch behind wooded down sport camp moment deal yell outside keenly interested altho perhaps ought anyhow prefer inside saw race ago boot race fellow boot pile together heap mixed race search boot grab boot ten yard scramble suddenly belong fling disgust rush probably wear somebody else race win real degree certainty often mention every tea visitor delight big photo part holiday early weather beautifully fine always need holiday dream dream forrader\n",
      "\n",
      "Lowest vocab size is 6\n",
      "CORRESPONDING LETTER:\n",
      ">> 21st chance card 21 chance censor delay\n",
      "\n",
      "---\n",
      "\n",
      "Average type-token ratio is 0.86\n",
      "\n",
      "Highest type-token ratio is 1.0\n",
      "CORRESPONDING LETTER:\n",
      ">> might merry bright hearted yet hunt receive card alright must thank longmoor sign move cook mate work hard xmas sapper hodges\n",
      "\n",
      "Lowest type-token ratio is 0.41\n",
      "CORRESPONDING LETTER:\n",
      ">> 16th oct content food 16.10.18 thurloe contain list weigh approximately 15 lb pack sometimes content slightly alter b lb beef 1 ½ lb biscuit ½ lb vegetable ½ lb cocoa 1 lb tin ration 1 lb milk ½ lb tea 1 lb lyles syrup 1 lb tin milk 1 lb rice date ½ lb dripping margarine 1 pot meat 1 lb tin jam 1 tablet soap 1 ½ lb biscuits 1 lb tin ration 1 pkt quaker oat grape nut milk pudding 1 lb tin sausages 50 cigarette 1 lb sugar 1 tin sardine 1 lb suet pudding 1 tablet soap ½ lb chocolate 1 lb tin herrings 1lb tin veal ham beef loaf 1 lb bean 1 pkt quaker oat grape nut milk pudding ¼ lb cocoa oz tobacco ½ bacon 1 lb corn beef c 1 lb beef 1 lb tin ration 1 lb bacon 1 lb tin beef 1 lb bean 1 lb tin irish stew haricot mutton ¼ lb tea 1 lb biscuit 1 lb tin milk 1 lb tin jam ½ lb tin beef ham veal loaf 1 lb tin pork bean 1 lb biscuit ¼ lb tea 1 lb ration 1 lb sugar ½ lb tin drip margarine 1 lb tin meat potato pudding tripe onion 1 lb rice sago tapioca ½ lb cheese 1 lb tin jam tin fruit date 1 tin sardine 1 pot meat ½ lb tin margarine 100 cigarette 50 cigarette 1 tablet soap 1 tablet soap ½ lb veal ham beef loaf ½ lb bacon list store discovery coat muriel wide afraid great part undone fortunately undo remember trafalgar square represent battle field dugout buy loan million money require everyone money add postscript censor cut approve ever watford pratt censor mean add something saucy end minnie capel eastbourne weather ps ally allow armistice unconditional surrender country surrender thus apparently others consider\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of letters: \", DATA_SIZE, \"\\n\\n---\")        \n",
    "\n",
    "# i could make a box plot???\n",
    "for measure, vals in size_measures.items():\n",
    "    print(f\"\\nAverage {measure} is {round(sum(vals) / DATA_SIZE, 2)}\")\n",
    "    print(f\"\\nHighest {measure} is {max(vals)}\")\n",
    "    print(\"CORRESPONDING LETTER:\\n>>\", letters[vals.index(max(vals))])\n",
    "    print(f\"\\nLowest {measure} is {min(vals)}\")\n",
    "    print(\"CORRESPONDING LETTER:\\n>>\", letters[vals.index(min(vals))])\n",
    "    print(\"\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb3c515-4859-41d6-938c-fa6c48d081c1",
   "metadata": {},
   "source": [
    "<h4>OBSERVATIONS</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a838b9b-0c03-4ca9-bb55-92ccdc37646a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 most common words (after removing top 0.02%)\n",
      "since:           129\n",
      "receive:         125\n",
      "month:           121\n",
      "lot:             119\n",
      "new:             119\n",
      "many:            118\n",
      "tea:             118\n",
      "hour:            117\n",
      "order:           117\n",
      "present:         116\n",
      "suppose:         115\n",
      "use:             112\n",
      "something:       111\n",
      "sleep:           111\n",
      "holiday:         110\n",
      "usual:           110\n",
      "however:         109\n",
      "ever:            109\n",
      "always:          109\n",
      "whole:           108\n",
      "road:            108\n",
      "paper:           107\n",
      "july:            107\n",
      "balham:          106\n",
      "thank:           105\n",
      "bit:             104\n",
      "till:            104\n",
      "fire:            103\n",
      "weather:         103\n",
      "part:            101\n",
      "\n",
      "10 least common words\n",
      "b.e.f.:          1\n",
      "j.:              1\n",
      "lance:           1\n",
      "3343:            1\n",
      "non-fighting:    1\n",
      "a.p.c:           1\n",
      "ernie:           1\n",
      "redford:         1\n",
      "girl:            1\n",
      "cowan:           1\n",
      "morris:          1\n",
      "f.r:             1\n",
      "agreement:       1\n",
      "them…kind:       1\n",
      "theatrical:      1\n",
      "electrical:      1\n",
      "ok.:             1\n",
      "noncommissioned: 1\n",
      "4ft:             1\n",
      "2:               1\n",
      "skilling:        1\n",
      "h.a:             1\n",
      "s.s.:            1\n",
      "a.p:             1\n",
      "somme:           1\n",
      "boring:          1\n",
      "electrician:     1\n",
      "apparatus:       1\n",
      "room:            1\n",
      "tuition:         1\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n10 most common words (after removing top {REMOVE}%)\")\n",
    "for word, freq in total_freq.most_common(30):\n",
    "    print(f\"{word}:{' ' * (16 - len(word))}{freq}\")\n",
    "\n",
    "print(\"\\n10 least common words\")\n",
    "for word, freq in list(reversed(total_freq.most_common()))[:30]:\n",
    "    print(f\"{word}:{' ' * (16 - len(word))}{freq}\")\n",
    "\n",
    "# make bar chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03978476-0059-4b12-8c26-4a808b05df16",
   "metadata": {},
   "source": [
    "<h4>OBSERVATIONS</h4>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
