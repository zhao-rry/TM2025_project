{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95155ead-60d5-449b-88e5-83af88a96aff",
   "metadata": {},
   "source": [
    "<h1> Quantitative Analysis </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3476e65f-223d-46e7-b9b3-65378a138c10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dd8f20-6949-4bd4-a118-56344e73210f",
   "metadata": {},
   "source": [
    "<h2> Preprocessing Steps </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59feb18b-ca3e-44f3-a6b6-5d423176a308",
   "metadata": {},
   "source": [
    "<h4>Step 1</h4>\n",
    "\n",
    "Read data into Pandas dataframe and examine the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d4419ed-79bf-462f-8d79-836b7ac158ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   letter  year language\n",
      "arc71   Mercredi 29 septembre 1915\\nMa chère Louisette...  1915   french\n",
      "hl_01   Magnac Laval\\n\\nChère épouse et parents,\\n\\nAu...     0   french\n",
      "hl_02   Correspondance militaire adressée à monsieur J...  1914   french\n",
      "hl_03a  Chère femme, mes deux gosses ainsi que toute m...  1914   french\n",
      "hl_03b  Chers Mère et frère,\\n\\nTout ce que je vous re...  1914   french \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 78 entries, arc71 to new3\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   letter    78 non-null     object\n",
      " 1   year      78 non-null     int64 \n",
      " 2   language  78 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 4.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "RAW_FILE_PATH = Path.cwd() / \"ww1letters.json\"\n",
    "RAW_METADATA_PATH = Path.cwd() / \"index.csv\"\n",
    "\n",
    "# we should include metadata such as the year, language, author\n",
    "with open(RAW_FILE_PATH, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "    indices = json_data.keys()\n",
    "    letters = json_data.values()\n",
    "    data_df = pd.DataFrame({\n",
    "        \"letter\": letters\n",
    "    }, index = indices)\n",
    "\n",
    "with open(RAW_METADATA_PATH, 'r') as f:\n",
    "    metadata_df = pd.read_csv(f)\n",
    "    metadata_df.index = metadata_df[\"letter_key\"]\n",
    "\n",
    "data_df = data_df.join(\n",
    "    metadata_df[[\"year\", \"language\"]],\n",
    ")\n",
    "data_df[\"year\"] = data_df[\"year\"].fillna(0)\n",
    "data_df = data_df.astype({\"year\": \"int64\"})\n",
    "data_df[\"language\"] = data_df[\"language\"].fillna(\"english\")\n",
    "\n",
    "print(data_df.head(), \"\\n\")\n",
    "print(data_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fade4a8-43fd-46d4-bac1-d2e88ee729a7",
   "metadata": {},
   "source": [
    "<h4>Step 2</h4>\n",
    "\n",
    "Retrieve English letters and apply tokenisation, lemmatisation, other steps?\n",
    "\n",
    "How to deal with French letters?\n",
    "\n",
    "We'll also explain more steps here.\n",
    "\n",
    "credits to notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7177ee4a-6f91-4321-b42d-33510e3f966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "\n",
    "STEMMER = nltk.stem.WordNetLemmatizer()\n",
    "un_to_wn_map = {\"VERB\" : wordnet.VERB,\n",
    "                \"NOUN\" : wordnet.NOUN,\n",
    "                \"ADJ\" : wordnet.ADJ,\n",
    "                \"ADV\" : wordnet.ADV}\n",
    "STOP = set(stopwords.words(\"english\")).union({'’', '“', '”', '[', ']', '…'})\n",
    "\n",
    "english_letters = data_df[\"letter\"][data_df[\"language\"] == \"english\"]\n",
    "# print(english_letters)\n",
    "\n",
    "letters_preprocessed = []\n",
    "for letter in english_letters:\n",
    "    letter_tokens = nltk.tokenize.word_tokenize(letter.lower())\n",
    "    letter_tagged = nltk.pos_tag(letter_tokens, tagset = \"universal\")\n",
    "    letter_lemmas = []\n",
    "    \n",
    "    for (token, pos) in letter_tagged:\n",
    "        if token not in STOP:\n",
    "            if pos in un_to_wn_map.keys():\n",
    "                letter_lemmas.append(STEMMER.lemmatize(token,\n",
    "                                                       pos = un_to_wn_map[pos]))\n",
    "            elif token.isalnum():\n",
    "                letter_lemmas.append(STEMMER.lemmatize(token))\n",
    "\n",
    "    letters_preprocessed.append(' '.join(letter_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60777cdd-416a-4aa4-a1d3-1951b75aa790",
   "metadata": {},
   "source": [
    "<h4>Step 3</h4>\n",
    "\n",
    "Replace the letters in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5295b135-33ca-4a85-9554-9fbd5f1edfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     letter  year\n",
      "na_uk_01  dear mr welsh matter hour go trench eight day ...  1915\n",
      "na_uk_02  dear nic boyce thanks much interesting letter…...  1915\n",
      "na_uk_03  dear lack many thanks photograph receive think...  1915\n",
      "na_uk_04  dear bert line let know alright hop same… pres...  1915\n",
      "na_uk_05  dear mr hunt please accept convey gentleman be...  1915 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 58 entries, na_uk_01 to new3\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   letter  58 non-null     object\n",
      " 1   year    58 non-null     int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "english_data_df = data_df[data_df[\"language\"] == \"english\"].copy()\n",
    "english_data_df = english_data_df.drop(columns = [\"language\"])\n",
    "english_data_df[\"letter\"] = letters_preprocessed\n",
    "\n",
    "print(english_data_df.head(), \"\\n\")\n",
    "print(english_data_df.info())\n",
    "\n",
    "english_data_df.to_csv(Path.cwd() / \"preprocessed_letters.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5196b4-b9aa-466c-85ca-32d71fb150dd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb5d805-5363-4fa5-9751-a14401db3e74",
   "metadata": {},
   "source": [
    "<h2>General Statistics</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e353719-efb3-423a-803f-c3f28b3203b5",
   "metadata": {},
   "source": [
    "<h4>ugh</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a246d878-f2be-42de-91f5-bb1f85be52ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "english_letters = pd.read_csv(\"preprocessed_letters.csv\")[\"letter\"]\n",
    "\n",
    "def letter_stats(letter: list[str]) -> float:\n",
    "    \"\"\"\n",
    "    Takes in a letter and calculates its key characteristics\n",
    "    \"\"\"\n",
    "    text_size = len(letter)\n",
    "    vocab_size = len(set(letter))\n",
    "    ttr = vocab_size / text_size\n",
    "\n",
    "    return ttr\n",
    "\n",
    "def show_letter_stats(letters: list[list[str]]) -> (list[float]):\n",
    "    \"\"\"\n",
    "    Show stuff\n",
    "    \"\"\"\n",
    "    for letter in letters:\n",
    "        ttr = letter_stats(letter)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
