{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95155ead-60d5-449b-88e5-83af88a96aff",
   "metadata": {},
   "source": [
    "<h1> Quantitative Analysis </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3476e65f-223d-46e7-b9b3-65378a138c10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dd8f20-6949-4bd4-a118-56344e73210f",
   "metadata": {},
   "source": [
    "<h2> Preprocessing Steps </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59feb18b-ca3e-44f3-a6b6-5d423176a308",
   "metadata": {},
   "source": [
    "<h4>Step 1</h4>\n",
    "\n",
    "Read data into Pandas dataframe and examine the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d4419ed-79bf-462f-8d79-836b7ac158ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/zhaorry/Desktop/university/text_mining_project/data/ww1letters.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m KAGGLE_FILE_PATH \u001b[38;5;241m=\u001b[39m DATA_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mww1letters.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m KAGGLE_METADATA_PATH \u001b[38;5;241m=\u001b[39m DATA_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(KAGGLE_FILE_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     11\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     12\u001b[0m     indices \u001b[38;5;241m=\u001b[39m json_data\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/zhaorry/Desktop/university/text_mining_project/data/ww1letters.json'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path.cwd() / \"data\"\n",
    "KAGGLE_FILE_PATH = DATA_DIR / \"ww1letters.json\"\n",
    "KAGGLE_METADATA_PATH = DATA_DIR / \"index.csv\"\n",
    "\n",
    "with open(KAGGLE_FILE_PATH, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "    indices = json_data.keys()\n",
    "    letters = json_data.values()\n",
    "    kaggle_df = pd.DataFrame({\n",
    "        \"letter\": letters\n",
    "    }, index = indices)\n",
    "\n",
    "with open(KAGGLE_METADATA_PATH, 'r') as f:\n",
    "    metadata_df = pd.read_csv(f)\n",
    "    metadata_df.index = metadata_df[\"letter_key\"]\n",
    "\n",
    "kaggle_df = kaggle_df.join(\n",
    "    metadata_df[[\"year\", \"language\"]],\n",
    ")\n",
    "kaggle_df[\"year\"] = kaggle_df[\"year\"].fillna(0)\n",
    "kaggle_df = kaggle_df.astype({\"year\": \"int64\"})\n",
    "kaggle_df[\"language\"] = kaggle_df[\"language\"].fillna(\"english\")\n",
    "\n",
    "print(kaggle_df.head(), \"\\n\")\n",
    "print(kaggle_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc0abd5-e98f-473f-bffc-ac3639c92643",
   "metadata": {},
   "outputs": [],
   "source": [
    "LETTERS400_PATH = DATA_DIR / \"400_letters.json\"\n",
    "\n",
    "with open(LETTERS400_PATH, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "    print(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fade4a8-43fd-46d4-bac1-d2e88ee729a7",
   "metadata": {},
   "source": [
    "<h4>Step 2</h4>\n",
    "\n",
    "Retrieve English letters and apply tokenisation, lemmatisation, other steps?\n",
    "\n",
    "How to deal with French letters?\n",
    "\n",
    "We'll also explain more steps here.\n",
    "\n",
    "credits to notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7177ee4a-6f91-4321-b42d-33510e3f966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "\n",
    "STEMMER = nltk.stem.WordNetLemmatizer()\n",
    "un_to_wn_map = {\"VERB\" : wordnet.VERB,\n",
    "                \"NOUN\" : wordnet.NOUN,\n",
    "                \"ADJ\" : wordnet.ADJ,\n",
    "                \"ADV\" : wordnet.ADV}\n",
    "STOP = set(stopwords.words(\"english\")).union({'’', '“', '”', '[', ']', '…'})\n",
    "\n",
    "english_kaggle = kaggle_df[\"letter\"][kaggle_df[\"language\"] == \"english\"]\n",
    "# print(english_kaggle)\n",
    "\n",
    "def preprocess(df: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Takes a series of letters and preprocesses them\n",
    "    by applying tokenisation, \n",
    "    \"\"\"\n",
    "    letters_preprocessed = []\n",
    "    for letter in df:\n",
    "        letter_tokens = nltk.tokenize.word_tokenize(letter.lower())\n",
    "        letter_tagged = nltk.pos_tag(letter_tokens, tagset = \"universal\")\n",
    "        letter_lemmas = []\n",
    "        \n",
    "        for (token, pos) in letter_tagged:\n",
    "            if token not in STOP:\n",
    "                if pos in un_to_wn_map.keys():\n",
    "                    letter_lemmas.append(STEMMER.lemmatize(token,\n",
    "                                                           pos = un_to_wn_map[pos]))\n",
    "                elif token.isalnum():\n",
    "                    letter_lemmas.append(STEMMER.lemmatize(token))\n",
    "    \n",
    "        letters_preprocessed.append(' '.join(letter_lemmas))\n",
    "\n",
    "    return letters_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60777cdd-416a-4aa4-a1d3-1951b75aa790",
   "metadata": {},
   "source": [
    "<h4>Step 3</h4>\n",
    "\n",
    "Replace the letters in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5295b135-33ca-4a85-9554-9fbd5f1edfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     letter  year\n",
      "na_uk_01  dear mr welsh matter hour go trench eight day ...  1915\n",
      "na_uk_02  dear nic boyce thanks much interesting letter…...  1915\n",
      "na_uk_03  dear lack many thanks photograph receive think...  1915\n",
      "na_uk_04  dear bert line let know alright hop same… pres...  1915\n",
      "na_uk_05  dear mr hunt please accept convey gentleman be...  1915 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 58 entries, na_uk_01 to new3\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   letter  58 non-null     object\n",
      " 1   year    58 non-null     int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "english_data_df = data_df[data_df[\"language\"] == \"english\"].copy()\n",
    "english_data_df = english_data_df.drop(columns = [\"language\"])\n",
    "english_data_df[\"letter\"] = letters_preprocessed\n",
    "\n",
    "print(english_data_df.head(), \"\\n\")\n",
    "print(english_data_df.info())\n",
    "\n",
    "english_data_df.to_csv(DATA_DIR / \"preprocessed_letters.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5196b4-b9aa-466c-85ca-32d71fb150dd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb5d805-5363-4fa5-9751-a14401db3e74",
   "metadata": {},
   "source": [
    "<h2>General Statistics</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e353719-efb3-423a-803f-c3f28b3203b5",
   "metadata": {},
   "source": [
    "<h4>ugh</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a246d878-f2be-42de-91f5-bb1f85be52ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "english_letters = pd.read_csv(DATA_DIR / \"preprocessed_letters.csv\")[\"letter\"]\n",
    "\n",
    "def letter_stats(letter: list[str]) -> float:\n",
    "    \"\"\"\n",
    "    Takes in a letter and calculates its key characteristics\n",
    "    \"\"\"\n",
    "    text_size = len(letter)\n",
    "    vocab_size = len(set(letter))\n",
    "    ttr = vocab_size / text_size\n",
    "\n",
    "    return ttr\n",
    "\n",
    "def show_letter_stats(letters: list[list[str]]) -> (list[float]):\n",
    "    \"\"\"\n",
    "    Show stuff\n",
    "    \"\"\"\n",
    "    for letter in letters:\n",
    "        ttr = letter_stats(letter)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
